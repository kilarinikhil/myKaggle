{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using cnn for mnist dataset\n\n**libraries used**\n\n* numpy\n* pandas\n* tensorflow"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## define hyper parameters\n\n- learningRate : define your learning rate for Adam optimizer\n- shuffleSize  : define your shuffle size for dataset, any number greater than the size of dataset is preferred\n- batchSize    : define your batch size for training data\n- displayStep  : define after how many steps you want to display metrics, one step means one batch of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper parameters\nlearningRate = 0.001\nshuffleSize = 42000\nbatchSize = 256\nnumSteps = 1500\ndisplayStep = 20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# dataset\n\n- import dataset from keras library\n- convert xTrain and xTest into float32\n- normalize data by dividing with 255\n- reshape dataset such that each input tensor is of shape 28 * 28 * 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n# Convert to float32.\nxTrain, xTest = np.array(xTrain, np.float32), np.array(xTest, np.float32)\n# Normalize images value from [0, 255] to [0, 1].\nxTrain, xTest = xTrain / 255., xTest / 255.\n# reshape matrix\nxTrain = np.reshape(xTrain,(-1,28,28,1))\nxTest = np.reshape(xTest,(-1,28,28,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## manage your dataset\n\n- create a dataset using imported data from mnist\n- shuffle(shuffleSize) shuffles the dataset\n- batch(batchSize) divides the dataset into batches each containing \"batchSize\" inputs\n- repeat(-1) repeats the dataset infinite times"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"trainData = tf.data.Dataset.from_tensor_slices((xTrain,yTrain))\ntrainData = trainData.repeat(-1).shuffle(shuffleSize).batch(batchSize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## architecture\n\n- using keras.Layers import required layers\n- Conv2D is convolutional layer\n- MaxPool2D is maxpooling layer\n- Dropout layer is used only during training\n- Flatten layer flattens the previous layer\n- Dense is a classic neural network layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class cnn(Model):\n    def __init__(self):\n        super(cnn,self).__init__()\n        self.c1 = Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation = 'relu', input_shape = (28,28,1))\n        self.c2 = Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation = 'relu')\n        self.mp1 = MaxPool2D(pool_size = (2,2))\n        self.dout1 = Dropout(0.25)\n        self.c3 = Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu')\n        self.c4 = Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu')\n        self.mp2 = MaxPool2D(pool_size = (2,2), strides = (2,2))\n        self.dout2 = Dropout(0.25)\n        self.flatten  = Flatten()\n        self.d1 = Dense(256,activation = 'relu')\n        self.dout3 = Dropout(0.5)\n        self.d2 = Dense(10)\n        \n    def call(self,x,is_training = False):\n        x = self.c1(x)\n        x = self.c2(x)\n        x = self.mp1(x)\n        x = self.dout1(x,training = is_training)\n        x = self.c3(x)\n        x = self.c4(x)\n        x = self.mp2(x)\n        x = self.dout2(x,training = is_training)\n        x = self.flatten(x)\n        x = self.d1(x)\n        x = self.dout3(x,training = is_training)\n        x = self.d2(x)\n        if is_training:\n            return x\n        return tf.nn.softmax(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## create a cnn object"},{"metadata":{"trusted":true},"cell_type":"code","source":"myCNN = cnn()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def lossOp(pred,true):\n    true = tf.cast(true,dtype = tf.int64)\n    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(true,pred))\n\ndef accuracy(pred,true):\n    pred = tf.math.argmax(pred,1)\n    acc = tf.keras.metrics.Accuracy()\n    _ = acc.update_state(pred,true)\n    return acc.result().numpy()\n\noptimizer = tf.optimizers.Adam(learningRate)\n\ndef optimize(x,y):\n    with tf.GradientTape() as g:\n        pred = myCNN(x,is_training = True)\n        loss = lossOp(pred,y)\n    trainVars = myCNN.trainable_variables\n    gradients = g.gradient(loss,trainVars)\n    optimizer.apply_gradients(zip(gradients,trainVars))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for step,(batchX,batchY) in enumerate(trainData.take(numSteps),1):\n    optimize(batchX,batchY)\n    if step % displayStep == 0:\n        pred = myCNN(batchX)\n        loss = lossOp(pred,batchY)\n        acc = accuracy(pred,batchY)\n        print(\"Step: %i, loss: %f, accu: %f\"%(step,loss,acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = myCNN(xTest,is_training = False)\naccu = accuracy(pred,yTest)\nloss = lossOp(pred,yTest)\nprint(\"Cross validation loss: %f, accuracy: %f\"%(loss,accu))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDF = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nxTest = testDF.to_numpy()\nxTest = np.array(xTest,dtype = np.float32)/255.\nxTest = np.reshape(xTest,(-1,28,28,1))\npred = myCNN(xTest,is_training = False)\npred = tf.argmax(pred,1).numpy()\nId = np.arange(1,28001)\nout = pd.DataFrame({'ImageId' : Id, 'Label' : pred})\nout.to_csv('outputFile.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}